{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pandas.api.types import is_string_dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({\"A\": [\"a\", \"b\", \"NaN\", \"a\"]})\n",
    "PATH = \"data/bulldozers/\"\n",
    "df_raw = pd.read_csv(f'{PATH}Train.csv', low_memory=False, \n",
    "                     parse_dates=[\"saledate\"])\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#from .imports import *\n",
    "import numpy as np\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute._base import SimpleImputer as Imputer\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from sklearn.ensemble import forest\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def set_plot_sizes(sml, med, big):\n",
    "    plt.rc('font', size=sml)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=sml)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=sml)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=big)  # fontsize of the figure title\n",
    "\n",
    "def parallel_trees(m, fn, n_jobs=8):\n",
    "        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\n",
    "\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    \"\"\" Draws a representation of a random forest in IPython.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    t: The tree you wish to draw\n",
    "    df: The data used to train the tree. This is used to get the names of the features.\n",
    "    \"\"\"\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))\n",
    "\n",
    "def combine_date(years, months=1, days=1, weeks=None, hours=None, minutes=None,\n",
    "              seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):\n",
    "    years = np.asarray(years) - 1970\n",
    "    months = np.asarray(months) - 1\n",
    "    days = np.asarray(days) - 1\n",
    "    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',\n",
    "             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')\n",
    "    vals = (years, months, days, weeks, hours, minutes, seconds,\n",
    "            milliseconds, microseconds, nanoseconds)\n",
    "    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)\n",
    "               if v is not None)\n",
    "\n",
    "def get_sample(df,n):\n",
    "    \"\"\" Gets a random sample of n rows from df, without replacement.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame, that you wish to sample from.\n",
    "    n: The number of rows you wish to sample.\n",
    "    Returns:\n",
    "    --------\n",
    "    return value: A random sample of n rows of df.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    >>> get_sample(df, 2)\n",
    "       col1 col2\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    \"\"\"\n",
    "    idxs = sorted(np.random.permutation(len(df))[:n])\n",
    "    return df.iloc[idxs].copy()\n",
    "\n",
    "def add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):  \n",
    "    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n",
    "    the information from the date. This applies changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame. df gain several new columns.\n",
    "    fldname: A string or list of strings that is the name of the date column you wish to expand.\n",
    "        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n",
    "    drop: If true then the original date column will be removed.\n",
    "    time: If true time features: Hour, Minute, Second will be added.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n",
    "    >>> df\n",
    "        A\n",
    "    0   2000-03-11\n",
    "    1   2000-03-12\n",
    "    2   2000-03-13\n",
    "    >>> add_datepart(df, 'A')\n",
    "    >>> df\n",
    "        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n",
    "    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n",
    "    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n",
    "    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n",
    "    >>>df2 = pd.DataFrame({'start_date' : pd.to_datetime(['3/11/2000','3/13/2000','3/15/2000']),\n",
    "                            'end_date':pd.to_datetime(['3/17/2000','3/18/2000','4/1/2000'],infer_datetime_format=True)})\n",
    "    >>>df2\n",
    "        start_date  end_date    \n",
    "    0   2000-03-11  2000-03-17\n",
    "    1   2000-03-13  2000-03-18\n",
    "    2   2000-03-15  2000-04-01\n",
    "    >>>add_datepart(df2,['start_date','end_date'])\n",
    "    >>>df2\n",
    "        start_Year  start_Month start_Week  start_Day   start_Dayofweek start_Dayofyear start_Is_month_end  start_Is_month_start    start_Is_quarter_end    start_Is_quarter_start  start_Is_year_end   start_Is_year_start start_Elapsed   end_Year    end_Month   end_Week    end_Day end_Dayofweek   end_Dayofyear   end_Is_month_end    end_Is_month_start  end_Is_quarter_end  end_Is_quarter_start    end_Is_year_end end_Is_year_start   end_Elapsed\n",
    "    0   2000        3           10          11          5               71              False               False                   False                   False                   False               False               952732800       2000        3           11          17      4               77              False               False               False               False                   False           False               953251200\n",
    "    1   2000        3           11          13          0               73              False               False                   False                   False                   False               False               952905600       2000        3           11          18      5               78              False               False               False               False                   False           False               953337600\n",
    "    2   2000        3           11          15          2               75              False               False                   False                   False                   False               False               953078400       2000        4           13          1       5               92              False               True                False               True                    False           False               954547200\n",
    "    \"\"\"\n",
    "    if isinstance(fldnames,str): \n",
    "        fldnames = [fldnames]\n",
    "    for fldname in fldnames:\n",
    "        fld = df[fldname]\n",
    "        fld_dtype = fld.dtype\n",
    "        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            fld_dtype = np.datetime64\n",
    "\n",
    "        if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "        targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "        if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "\n",
    "def is_date(x): return np.issubdtype(x.dtype, np.datetime64)\n",
    "\n",
    "def train_cats(df):\n",
    "    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n",
    "    categorical values. This applies the changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. Any columns of strings will be changed to\n",
    "        categorical values.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category\n",
    "    \"\"\"\n",
    "    for n,c in df.items():\n",
    "        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
    "\n",
    "def apply_cats(df, trn):\n",
    "    \"\"\"Changes any columns of strings in df into categorical variables using trn as\n",
    "    a template for the category codes.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. Any columns of strings will be changed to\n",
    "        categorical values. The category codes are determined by trn.\n",
    "    trn: A pandas dataframe. When creating a category for df, it looks up the\n",
    "        what the category's code were in trn and makes those the category codes\n",
    "        for df.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category {a : 1, b : 2}\n",
    "    >>> df2 = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['b', 'a', 'a']})\n",
    "    >>> apply_cats(df2, df)\n",
    "           col1 col2\n",
    "        0     1    b\n",
    "        1     2    a\n",
    "        2     3    a\n",
    "    now the type of col is category {a : 1, b : 2}\n",
    "    \"\"\"\n",
    "    for n,c in df.items():\n",
    "        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n",
    "            df[n] = c.astype('category').cat.as_ordered()\n",
    "            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)\n",
    "\n",
    "def fix_missing(df, col, name, na_dict):\n",
    "    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n",
    "    which specifies if the data was missing.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: The data frame that will be changed.\n",
    "    col: The column of data to fix by filling in missing data.\n",
    "    name: The name of the new filled column in df.\n",
    "    na_dict: A dictionary of values to create na's of and the value to insert. If\n",
    "        name is not a key of na_dict the median will fill any missing data. Also\n",
    "        if name is not a key of na_dict and there is no missing data in col, then\n",
    "        no {name}_na column is not created.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col1'], 'col1', {})\n",
    "    >>> df\n",
    "       col1 col2 col1_na\n",
    "    0     1    5   False\n",
    "    1     2    2    True\n",
    "    2     3    2   False\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col2'], 'col2', {})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n",
    "    >>> df\n",
    "       col1 col2 col1_na\n",
    "    0     1    5   False\n",
    "    1   500    2    True\n",
    "    2     3    2   False\n",
    "    \"\"\"\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum() or (name in na_dict):\n",
    "            df[name+'_na'] = pd.isnull(col)\n",
    "            filler = na_dict[name] if name in na_dict else col.median()\n",
    "            df[name] = col.fillna(filler)\n",
    "            na_dict[name] = filler\n",
    "    return na_dict\n",
    "\n",
    "def numericalize(df, col, name, max_n_cat):\n",
    "    \"\"\" Changes the column col from a categorical type to it's integer codes.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. df[name] will be filled with the integer codes from\n",
    "        col.\n",
    "    col: The column you wish to change into the categories.\n",
    "    name: The column name you wish to insert into df. This column will hold the\n",
    "        integer codes.\n",
    "    max_n_cat: If col has more categories than max_n_cat it will not change the\n",
    "        it to its integer codes. If max_n_cat is None, then col will always be\n",
    "        converted.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category { a : 1, b : 2}\n",
    "    >>> numericalize(df, df['col2'], 'col3', None)\n",
    "       col1 col2 col3\n",
    "    0     1    a    1\n",
    "    1     2    b    2\n",
    "    2     3    a    1\n",
    "    \"\"\"\n",
    "    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n",
    "        df[name] = pd.Categorical(col).codes+1\n",
    "\n",
    "def scale_vars(df, mapper):\n",
    "    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n",
    "    if mapper is None:\n",
    "        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n",
    "        mapper = DataFrameMapper(map_f).fit(df)\n",
    "    df[mapper.transformed_names_] = mapper.transform(df)\n",
    "    return mapper\n",
    "\n",
    "def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n",
    "            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n",
    "    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n",
    "    changes the df into an entirely numeric dataframe. For each column of df \n",
    "    which is not in skip_flds nor in ignore_flds, na values are replaced by the\n",
    "    median value of the column.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: The data frame you wish to process.\n",
    "    y_fld: The name of the response variable\n",
    "    skip_flds: A list of fields that dropped from df.\n",
    "    ignore_flds: A list of fields that are ignored during processing.\n",
    "    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n",
    "    na_dict: a dictionary of na columns to add. Na columns are also added if there\n",
    "        are any missing values.\n",
    "    preproc_fn: A function that gets applied to df.\n",
    "    max_n_cat: The maximum number of categories to break into dummy values, instead\n",
    "        of integer codes.\n",
    "    subset: Takes a random subset of size subset from df.\n",
    "    mapper: If do_scale is set as True, the mapper variable\n",
    "        calculates the values used for scaling of variables during training time (mean and standard deviation).\n",
    "    Returns:\n",
    "    --------\n",
    "    [x, y, nas, mapper(optional)]:\n",
    "        x: x is the transformed version of df. x will not have the response variable\n",
    "            and is entirely numeric.\n",
    "        y: y is the response variable\n",
    "        nas: returns a dictionary of which nas it created, and the associated median.\n",
    "        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n",
    "        variables which is then used for scaling of during test-time.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category { a : 1, b : 2}\n",
    "    >>> x, y, nas = proc_df(df, 'col1')\n",
    "    >>> x\n",
    "       col2\n",
    "    0     1\n",
    "    1     2\n",
    "    2     1\n",
    "    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n",
    "                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n",
    "                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n",
    "    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n",
    "                          ([:children], StandardScaler())])\n",
    "    >>>round(fit_transform!(mapper, copy(data)), 2)\n",
    "    8x4 Array{Float64,2}:\n",
    "    1.0  0.0  0.0   0.21\n",
    "    0.0  1.0  0.0   1.88\n",
    "    0.0  1.0  0.0  -0.63\n",
    "    0.0  0.0  1.0  -0.63\n",
    "    1.0  0.0  0.0  -1.46\n",
    "    0.0  1.0  0.0  -0.63\n",
    "    1.0  0.0  0.0   1.04\n",
    "    0.0  0.0  1.0   0.21\n",
    "    \"\"\"\n",
    "    if not ignore_flds: ignore_flds=[]\n",
    "    if not skip_flds: skip_flds=[]\n",
    "    if subset: df = get_sample(df,subset)\n",
    "    else: df = df.copy()\n",
    "    ignored_flds = df.loc[:, ignore_flds]\n",
    "    df.drop(ignore_flds, axis=1, inplace=True)\n",
    "    if preproc_fn: preproc_fn(df)\n",
    "    if y_fld is None: y = None\n",
    "    else:\n",
    "        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n",
    "        y = df[y_fld].values\n",
    "        skip_flds += [y_fld]\n",
    "    df.drop(skip_flds, axis=1, inplace=True)\n",
    "\n",
    "    if na_dict is None: na_dict = {}\n",
    "    else: na_dict = na_dict.copy()\n",
    "    na_dict_initial = na_dict.copy()\n",
    "    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n",
    "    if len(na_dict_initial.keys()) > 0:\n",
    "        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n",
    "    if do_scale: mapper = scale_vars(df, mapper)\n",
    "    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    df = pd.concat([ignored_flds, df], axis=1)\n",
    "    res = [df, y, na_dict]\n",
    "    if do_scale: res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "def rf_feat_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "\n",
    "def set_rf_samples(n):\n",
    "    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n",
    "    n random rows.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "\n",
    "def reset_rf_samples():\n",
    "    \"\"\" Undoes the changes produced by set_rf_samples.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n",
    "\n",
    "def get_nn_mappers(df, cat_vars, contin_vars):\n",
    "    # Replace nulls with 0 for continuous, \"\" for categorical.\n",
    "    for v in contin_vars: df[v] = df[v].fillna(df[v].max()+100,)\n",
    "    for v in cat_vars: df[v].fillna('#NA#', inplace=True)\n",
    "\n",
    "    # list of tuples, containing variable and instance of a transformer for that variable\n",
    "    # for categoricals, use LabelEncoder to map to integers. For continuous, standardize\n",
    "    cat_maps = [(o, LabelEncoder()) for o in cat_vars]\n",
    "    contin_maps = [([o], StandardScaler()) for o in contin_vars]\n",
    "#from .imports import *\n",
    "import numpy\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute._base import SimpleImputer as Imputer\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from sklearn.ensemble import forest\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def set_plot_sizes(sml, med, big):\n",
    "    plt.rc('font', size=sml)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=sml)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=sml)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=big)  # fontsize of the figure title\n",
    "\n",
    "def parallel_trees(m, fn, n_jobs=8):\n",
    "        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\n",
    "\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    \"\"\" Draws a representation of a random forest in IPython.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    t: The tree you wish to draw\n",
    "    df: The data used to train the tree. This is used to get the names of the features.\n",
    "    \"\"\"\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))\n",
    "\n",
    "def combine_date(years, months=1, days=1, weeks=None, hours=None, minutes=None,\n",
    "              seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):\n",
    "    years = np.asarray(years) - 1970\n",
    "    months = np.asarray(months) - 1\n",
    "    days = np.asarray(days) - 1\n",
    "    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',\n",
    "             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')\n",
    "    vals = (years, months, days, weeks, hours, minutes, seconds,\n",
    "            milliseconds, microseconds, nanoseconds)\n",
    "    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)\n",
    "               if v is not None)\n",
    "\n",
    "def get_sample(df,n):\n",
    "    \"\"\" Gets a random sample of n rows from df, without replacement.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame, that you wish to sample from.\n",
    "    n: The number of rows you wish to sample.\n",
    "    Returns:\n",
    "    --------\n",
    "    return value: A random sample of n rows of df.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    >>> get_sample(df, 2)\n",
    "       col1 col2\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    \"\"\"\n",
    "    idxs = sorted(np.random.permutation(len(df))[:n])\n",
    "    return df.iloc[idxs].copy()\n",
    "\n",
    "def add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):  \n",
    "    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n",
    "    the information from the date. This applies changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame. df gain several new columns.\n",
    "    fldname: A string or list of strings that is the name of the date column you wish to expand.\n",
    "        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n",
    "    drop: If true then the original date column will be removed.\n",
    "    time: If true time features: Hour, Minute, Second will be added.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n",
    "    >>> df\n",
    "        A\n",
    "    0   2000-03-11\n",
    "    1   2000-03-12\n",
    "    2   2000-03-13\n",
    "    >>> add_datepart(df, 'A')\n",
    "    >>> df\n",
    "        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n",
    "    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n",
    "    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n",
    "    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n",
    "    >>>df2 = pd.DataFrame({'start_date' : pd.to_datetime(['3/11/2000','3/13/2000','3/15/2000']),\n",
    "                            'end_date':pd.to_datetime(['3/17/2000','3/18/2000','4/1/2000'],infer_datetime_format=True)})\n",
    "    >>>df2\n",
    "        start_date  end_date    \n",
    "    0   2000-03-11  2000-03-17\n",
    "    1   2000-03-13  2000-03-18\n",
    "    2   2000-03-15  2000-04-01\n",
    "    >>>add_datepart(df2,['start_date','end_date'])\n",
    "    >>>df2\n",
    "        start_Year  start_Month start_Week  start_Day   start_Dayofweek start_Dayofyear start_Is_month_end  start_Is_month_start    start_Is_quarter_end    start_Is_quarter_start  start_Is_year_end   start_Is_year_start start_Elapsed   end_Year    end_Month   end_Week    end_Day end_Dayofweek   end_Dayofyear   end_Is_month_end    end_Is_month_start  end_Is_quarter_end  end_Is_quarter_start    end_Is_year_end end_Is_year_start   end_Elapsed\n",
    "    0   2000        3           10          11          5               71              False               False                   False                   False                   False               False               952732800       2000        3           11          17      4               77              False               False               False               False                   False           False               953251200\n",
    "    1   2000        3           11          13          0               73              False               False                   False                   False                   False               False               952905600       2000        3           11          18      5               78              False               False               False               False                   False           False               953337600\n",
    "    2   2000        3           11          15          2               75              False               False                   False                   False                   False               False               953078400       2000        4           13          1       5               92              False               True                False               True                    False           False               954547200\n",
    "    \"\"\"\n",
    "    if isinstance(fldnames,str): \n",
    "        fldnames = [fldnames]\n",
    "    for fldname in fldnames:\n",
    "        fld = df[fldname]\n",
    "        fld_dtype = fld.dtype\n",
    "        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            fld_dtype = np.datetime64\n",
    "\n",
    "        if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "        targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "        if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "\n",
    "def is_date(x): return np.issubdtype(x.dtype, np.datetime64)\n",
    "\n",
    "def train_cats(df):\n",
    "    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n",
    "    categorical values. This applies the changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. Any columns of strings will be changed to\n",
    "        categorical values.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category\n",
    "    \"\"\"\n",
    "    for n,c in df.items():\n",
    "        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
    "\n",
    "def apply_cats(df, trn):\n",
    "    \"\"\"Changes any columns of strings in df into categorical variables using trn as\n",
    "    a template for the category codes.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. Any columns of strings will be changed to\n",
    "        categorical values. The category codes are determined by trn.\n",
    "    trn: A pandas dataframe. When creating a category for df, it looks up the\n",
    "        what the category's code were in trn and makes those the category codes\n",
    "        for df.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category {a : 1, b : 2}\n",
    "    >>> df2 = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['b', 'a', 'a']})\n",
    "    >>> apply_cats(df2, df)\n",
    "           col1 col2\n",
    "        0     1    b\n",
    "        1     2    a\n",
    "        2     3    a\n",
    "    now the type of col is category {a : 1, b : 2}\n",
    "    \"\"\"\n",
    "    for n,c in df.items():\n",
    "        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n",
    "            df[n] = c.astype('category').cat.as_ordered()\n",
    "            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)\n",
    "\n",
    "def fix_missing(df, col, name, na_dict):\n",
    "    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n",
    "    which specifies if the data was missing.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: The data frame that will be changed.\n",
    "    col: The column of data to fix by filling in missing data.\n",
    "    name: The name of the new filled column in df.\n",
    "    na_dict: A dictionary of values to create na's of and the value to insert. If\n",
    "        name is not a key of na_dict the median will fill any missing data. Also\n",
    "        if name is not a key of na_dict and there is no missing data in col, then\n",
    "        no {name}_na column is not created.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col1'], 'col1', {})\n",
    "    >>> df\n",
    "       col1 col2 col1_na\n",
    "    0     1    5   False\n",
    "    1     2    2    True\n",
    "    2     3    2   False\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col2'], 'col2', {})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n",
    "    >>> df\n",
    "       col1 col2 col1_na\n",
    "    0     1    5   False\n",
    "    1   500    2    True\n",
    "    2     3    2   False\n",
    "    \"\"\"\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum() or (name in na_dict):\n",
    "            df[name+'_na'] = pd.isnull(col)\n",
    "            filler = na_dict[name] if name in na_dict else col.median()\n",
    "            df[name] = col.fillna(filler)\n",
    "            na_dict[name] = filler\n",
    "    return na_dict\n",
    "\n",
    "def numericalize(df, col, name, max_n_cat):\n",
    "    \"\"\" Changes the column col from a categorical type to it's integer codes.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. df[name] will be filled with the integer codes from\n",
    "        col.\n",
    "    col: The column you wish to change into the categories.\n",
    "    name: The column name you wish to insert into df. This column will hold the\n",
    "        integer codes.\n",
    "    max_n_cat: If col has more categories than max_n_cat it will not change the\n",
    "        it to its integer codes. If max_n_cat is None, then col will always be\n",
    "        converted.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category { a : 1, b : 2}\n",
    "    >>> numericalize(df, df['col2'], 'col3', None)\n",
    "       col1 col2 col3\n",
    "    0     1    a    1\n",
    "    1     2    b    2\n",
    "    2     3    a    1\n",
    "    \"\"\"\n",
    "    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n",
    "        df[name] = pd.Categorical(col).codes+1\n",
    "\n",
    "def scale_vars(df, mapper):\n",
    "    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n",
    "    if mapper is None:\n",
    "        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n",
    "        mapper = DataFrameMapper(map_f).fit(df)\n",
    "    df[mapper.transformed_names_] = mapper.transform(df)\n",
    "    return mapper\n",
    "\n",
    "def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n",
    "            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n",
    "    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n",
    "    changes the df into an entirely numeric dataframe. For each column of df \n",
    "    which is not in skip_flds nor in ignore_flds, na values are replaced by the\n",
    "    median value of the column.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: The data frame you wish to process.\n",
    "    y_fld: The name of the response variable\n",
    "    skip_flds: A list of fields that dropped from df.\n",
    "    ignore_flds: A list of fields that are ignored during processing.\n",
    "    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n",
    "    na_dict: a dictionary of na columns to add. Na columns are also added if there\n",
    "        are any missing values.\n",
    "    preproc_fn: A function that gets applied to df.\n",
    "    max_n_cat: The maximum number of categories to break into dummy values, instead\n",
    "        of integer codes.\n",
    "    subset: Takes a random subset of size subset from df.\n",
    "    mapper: If do_scale is set as True, the mapper variable\n",
    "        calculates the values used for scaling of variables during training time (mean and standard deviation).\n",
    "    Returns:\n",
    "    --------\n",
    "    [x, y, nas, mapper(optional)]:\n",
    "        x: x is the transformed version of df. x will not have the response variable\n",
    "            and is entirely numeric.\n",
    "        y: y is the response variable\n",
    "        nas: returns a dictionary of which nas it created, and the associated median.\n",
    "        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n",
    "        variables which is then used for scaling of during test-time.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category { a : 1, b : 2}\n",
    "    >>> x, y, nas = proc_df(df, 'col1')\n",
    "    >>> x\n",
    "       col2\n",
    "    0     1\n",
    "    1     2\n",
    "    2     1\n",
    "    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n",
    "                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n",
    "                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n",
    "    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n",
    "                          ([:children], StandardScaler())])\n",
    "    >>>round(fit_transform!(mapper, copy(data)), 2)\n",
    "    8x4 Array{Float64,2}:\n",
    "    1.0  0.0  0.0   0.21\n",
    "    0.0  1.0  0.0   1.88\n",
    "    0.0  1.0  0.0  -0.63\n",
    "    0.0  0.0  1.0  -0.63\n",
    "    1.0  0.0  0.0  -1.46\n",
    "    0.0  1.0  0.0  -0.63\n",
    "    1.0  0.0  0.0   1.04\n",
    "    0.0  0.0  1.0   0.21\n",
    "    \"\"\"\n",
    "    if not ignore_flds: ignore_flds=[]\n",
    "    if not skip_flds: skip_flds=[]\n",
    "    if subset: df = get_sample(df,subset)\n",
    "    else: df = df.copy()\n",
    "    ignored_flds = df.loc[:, ignore_flds]\n",
    "    df.drop(ignore_flds, axis=1, inplace=True)\n",
    "    if preproc_fn: preproc_fn(df)\n",
    "    if y_fld is None: y = None\n",
    "    else:\n",
    "        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n",
    "        y = df[y_fld].values\n",
    "        skip_flds += [y_fld]\n",
    "    df.drop(skip_flds, axis=1, inplace=True)\n",
    "\n",
    "    if na_dict is None: na_dict = {}\n",
    "    else: na_dict = na_dict.copy()\n",
    "    na_dict_initial = na_dict.copy()\n",
    "    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n",
    "    if len(na_dict_initial.keys()) > 0:\n",
    "        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n",
    "    if do_scale: mapper = scale_vars(df, mapper)\n",
    "    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    df = pd.concat([ignored_flds, df], axis=1)\n",
    "    res = [df, y, na_dict]\n",
    "    if do_scale: res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "def rf_feat_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "\n",
    "def set_rf_samples(n):\n",
    "    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n",
    "    n random rows.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "\n",
    "def reset_rf_samples():\n",
    "    \"\"\" Undoes the changes produced by set_rf_samples.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n",
    "\n",
    "def get_nn_mappers(df, cat_vars, contin_vars):\n",
    "    # Replace nulls with 0 for continuous, \"\" for categorical.\n",
    "    for v in contin_vars: df[v] = df[v].fillna(df[v].max()+100,)\n",
    "    for v in cat_vars: df[v].fillna('#NA#', inplace=True)\n",
    "\n",
    "    # list of tuples, containing variable and instance of a transformer for that variable\n",
    "    # for categoricals, use LabelEncoder to map to integers. For continuous, standardize\n",
    "    cat_maps = [(o, LabelEncoder()) for o in cat_vars]\n",
    "    contin_maps = [([o], StandardScaler()) for o in contin_vars]\n",
    "\n",
    "def world():\n",
    "    print(\"Hello, tarekj!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Low\n",
       "1            Low\n",
       "2           High\n",
       "3           High\n",
       "4         Medium\n",
       "           ...  \n",
       "401120       NaN\n",
       "401121       NaN\n",
       "401122       NaN\n",
       "401123       NaN\n",
       "401124       NaN\n",
       "Name: UsageBand, Length: 401125, dtype: category\n",
       "Categories (3, object): ['High' < 'Low' < 'Medium']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cats(df_raw)\n",
    "df_raw.UsageBand  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['High', 'Low', 'Medium'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#is_string_dtype(df_raw.ProductGroup)\n",
    "#df_raw.UsageBand  \n",
    "df_raw.UsageBand.cat.categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c4fe8a8743ff>:1: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n",
      "  df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SalesID': dtype('int64'), 'SalePrice': dtype('int64'), 'MachineID': dtype('int64'), 'ModelID': dtype('int64'), 'datasource': dtype('int64'), 'auctioneerID': dtype('float64'), 'YearMade': dtype('int64'), 'MachineHoursCurrentMeter': dtype('float64'), 'UsageBand': CategoricalDtype(categories=['High', 'Medium', 'Low'], ordered=True), 'saledate': dtype('<M8[ns]'), 'fiModelDesc': CategoricalDtype(categories=['100C', '104', '1066', '1066E', '1080', '1080B', '1088',\n",
      "                  '1088CK', '1088LT', '1088TTL',\n",
      "                  ...\n",
      "                  'ZX600LC', 'ZX70', 'ZX75', 'ZX75US', 'ZX75USA', 'ZX80',\n",
      "                  'ZX800', 'ZX800LC', 'ZX80LCK', 'ZX850H'],\n",
      ", ordered=True), 'fiBaseModel': CategoricalDtype(categories=['10', '100', '104', '1066', '1080', '1088', '10DG', '11',\n",
      "                  '110', '1105',\n",
      "                  ...\n",
      "                  'ZX370', 'ZX450', 'ZX460', 'ZX50', 'ZX600', 'ZX70', 'ZX75',\n",
      "                  'ZX80', 'ZX800', 'ZX850'],\n",
      ", ordered=True), 'fiSecondaryDesc': CategoricalDtype(categories=[' MSR SPIN ACE', '#NAME?', '-2', '-3', '-5', '-5L', '-6',\n",
      "                  '-7', '0.7', '1',\n",
      "                  ...\n",
      "                  'XP', 'XT', 'Z', 'ZF', 'ZHS', 'ZHS G', 'ZT', 'ZTM', 'ZTS',\n",
      "                  'ZX'],\n",
      ", ordered=True), 'fiModelSeries': CategoricalDtype(categories=[' III', '#NAME?', '-1', '-1.50E+01', '-11', '-12', '-15',\n",
      "                  '-16', '-17', '-18',\n",
      "                  ...\n",
      "                  'V-2', 'VHP', 'VI', 'WT', 'WX', 'XLT', 'XT', 'ZHS', 'ZTS',\n",
      "                  'ZX'],\n",
      ", ordered=True), 'fiModelDescriptor': CategoricalDtype(categories=[' 14FT', ' LGP', ' SUPER', ' XLT', ' XT', ' ZX',\n",
      "                  '(BLADE RUNNER)', '1', '2', '2.00E+00',\n",
      "                  ...\n",
      "                  'XLVP', 'XP', 'XR', 'XT', 'XTV', 'XW', 'Y', 'Z', 'ZTS',\n",
      "                  'ZX'],\n",
      ", ordered=True), 'ProductSize': CategoricalDtype(categories=['Compact', 'Large', 'Large / Medium', 'Medium', 'Mini',\n",
      "                  'Small'],\n",
      ", ordered=True), 'fiProductClassDesc': CategoricalDtype(categories=['Backhoe Loader - 0.0 to 14.0 Ft Standard Digging Depth',\n",
      "                  'Backhoe Loader - 14.0 to 15.0 Ft Standard Digging Depth',\n",
      "                  'Backhoe Loader - 15.0 to 16.0 Ft Standard Digging Depth',\n",
      "                  'Backhoe Loader - 16.0 + Ft Standard Digging Depth',\n",
      "                  'Backhoe Loader - Unidentified',\n",
      "                  'Hydraulic Excavator, Track - 0.0 to 2.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 11.0 to 12.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 12.0 to 14.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 14.0 to 16.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 150.0 to 300.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 16.0 to 19.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 19.0 to 21.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 2.0 to 3.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 21.0 to 24.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 24.0 to 28.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 28.0 to 33.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 3.0 to 4.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 300.0 + Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 33.0 to 40.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 4.0 to 5.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 4.0 to 6.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 40.0 to 50.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 5.0 to 6.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 50.0 to 66.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 6.0 to 8.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 66.0 to 90.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 8.0 to 11.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - 90.0 to 150.0 Metric Tons',\n",
      "                  'Hydraulic Excavator, Track - Unidentified',\n",
      "                  'Hydraulic Excavator, Track - Unidentified (Compact Construction)',\n",
      "                  'Motorgrader - 130.0 to 145.0 Horsepower',\n",
      "                  'Motorgrader - 145.0 to 170.0 Horsepower',\n",
      "                  'Motorgrader - 170.0 to 200.0 Horsepower',\n",
      "                  'Motorgrader - 200.0 + Horsepower',\n",
      "                  'Motorgrader - 45.0 to 130.0 Horsepower',\n",
      "                  'Motorgrader - Unidentified',\n",
      "                  'Skid Steer Loader - 0.0 to 701.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 1251.0 to 1351.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 1601.0 to 1751.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 1751.0 to 2201.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 2201.0 to 2701.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 2701.0+ Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 701.0 to 976.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - 976.0 to 1251.0 Lb Operating Capacity',\n",
      "                  'Skid Steer Loader - Unidentified',\n",
      "                  'Track Type Tractor, Dozer - 105.0 to 130.0 Horsepower',\n",
      "                  'Track Type Tractor, Dozer - 130.0 to 160.0 Horsepower',\n",
      "                  'Track Type Tractor, Dozer - 160.0 to 190.0 Horsepower',\n",
      "                  'Track Type Tractor, Dozer - 190.0 to 260.0 Horsepower',\n",
      "                  'Track Type Tractor, Dozer - 20.0 to 75.0 Horsepower',\n",
      "                  'Track Type Tractor, Dozer - 260.0 + Horsepower',\n",
      "                  'Track Type Tractor, Dozer - 75.0 to 85.0 Horsepower',\n",
      "                  'Track Type Tractor, Dozer - 85.0 to 105.0 Horsepower',\n",
      "                  'Track Type Tractor, Dozer - Unidentified',\n",
      "                  'Wheel Loader - 0.0 to 40.0 Horsepower',\n",
      "                  'Wheel Loader - 100.0 to 110.0 Horsepower',\n",
      "                  'Wheel Loader - 1000.0 + Horsepower',\n",
      "                  'Wheel Loader - 110.0 to 120.0 Horsepower',\n",
      "                  'Wheel Loader - 120.0 to 135.0 Horsepower',\n",
      "                  'Wheel Loader - 135.0 to 150.0 Horsepower',\n",
      "                  'Wheel Loader - 150.0 to 175.0 Horsepower',\n",
      "                  'Wheel Loader - 175.0 to 200.0 Horsepower',\n",
      "                  'Wheel Loader - 200.0 to 225.0 Horsepower',\n",
      "                  'Wheel Loader - 225.0 to 250.0 Horsepower',\n",
      "                  'Wheel Loader - 250.0 to 275.0 Horsepower',\n",
      "                  'Wheel Loader - 275.0 to 350.0 Horsepower',\n",
      "                  'Wheel Loader - 350.0 to 500.0 Horsepower',\n",
      "                  'Wheel Loader - 40.0 to 60.0 Horsepower',\n",
      "                  'Wheel Loader - 500.0 to 1000.0 Horsepower',\n",
      "                  'Wheel Loader - 60.0 to 80.0 Horsepower',\n",
      "                  'Wheel Loader - 80.0 to 90.0 Horsepower',\n",
      "                  'Wheel Loader - 90.0 to 100.0 Horsepower',\n",
      "                  'Wheel Loader - Unidentified'],\n",
      ", ordered=True), 'state': CategoricalDtype(categories=['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
      "                  'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
      "                  'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas',\n",
      "                  'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
      "                  'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
      "                  'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
      "                  'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
      "                  'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
      "                  'Puerto Rico', 'Rhode Island', 'South Carolina',\n",
      "                  'South Dakota', 'Tennessee', 'Texas', 'Unspecified', 'Utah',\n",
      "                  'Vermont', 'Virginia', 'Washington', 'Washington DC',\n",
      "                  'West Virginia', 'Wisconsin', 'Wyoming'],\n",
      ", ordered=True), 'ProductGroup': CategoricalDtype(categories=['BL', 'MG', 'SSL', 'TEX', 'TTT', 'WL'], ordered=True), 'ProductGroupDesc': CategoricalDtype(categories=['Backhoe Loaders', 'Motor Graders', 'Skid Steer Loaders',\n",
      "                  'Track Excavators', 'Track Type Tractors', 'Wheel Loader'],\n",
      ", ordered=True), 'Drive_System': CategoricalDtype(categories=['All Wheel Drive', 'Four Wheel Drive', 'No',\n",
      "                  'Two Wheel Drive'],\n",
      ", ordered=True), 'Enclosure': CategoricalDtype(categories=['EROPS', 'EROPS AC', 'EROPS w AC', 'NO ROPS',\n",
      "                  'None or Unspecified', 'OROPS'],\n",
      ", ordered=True), 'Forks': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Pad_Type': CategoricalDtype(categories=['Grouser', 'None or Unspecified', 'Reversible', 'Street'], ordered=True), 'Ride_Control': CategoricalDtype(categories=['No', 'None or Unspecified', 'Yes'], ordered=True), 'Stick': CategoricalDtype(categories=['Extended', 'Standard'], ordered=True), 'Transmission': CategoricalDtype(categories=['AutoShift', 'Autoshift', 'Direct Drive', 'Hydrostatic',\n",
      "                  'None or Unspecified', 'Powershift', 'Powershuttle',\n",
      "                  'Standard'],\n",
      ", ordered=True), 'Turbocharged': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Blade_Extension': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Blade_Width': CategoricalDtype(categories=['12'', '13'', '14'', '16'', '<12'', 'None or Unspecified'], ordered=True), 'Enclosure_Type': CategoricalDtype(categories=['High Profile', 'Low Profile', 'None or Unspecified'], ordered=True), 'Engine_Horsepower': CategoricalDtype(categories=['No', 'Variable'], ordered=True), 'Hydraulics': CategoricalDtype(categories=['2 Valve', '3 Valve', '4 Valve', 'Auxiliary',\n",
      "                  'Base + 1 Function', 'Base + 2 Function',\n",
      "                  'Base + 3 Function', 'Base + 4 Function',\n",
      "                  'Base + 5 Function', 'Base + 6 Function',\n",
      "                  'None or Unspecified', 'Standard'],\n",
      ", ordered=True), 'Pushblock': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Ripper': CategoricalDtype(categories=['Multi Shank', 'None or Unspecified', 'Single Shank', 'Yes'], ordered=True), 'Scarifier': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Tip_Control': CategoricalDtype(categories=['None or Unspecified', 'Sideshift & Tip', 'Tip'], ordered=True), 'Tire_Size': CategoricalDtype(categories=['10 inch', '10\"', '13\"', '14\"', '15.5', '15.5\"', '17.5',\n",
      "                  '17.5\"', '20.5', '20.5\"', '23.1\"', '23.5', '23.5\"', '26.5',\n",
      "                  '29.5', '7.0\"', 'None or Unspecified'],\n",
      ", ordered=True), 'Coupler': CategoricalDtype(categories=['Hydraulic', 'Manual', 'None or Unspecified'], ordered=True), 'Coupler_System': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Grouser_Tracks': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Hydraulics_Flow': CategoricalDtype(categories=['High Flow', 'None or Unspecified', 'Standard'], ordered=True), 'Track_Type': CategoricalDtype(categories=['Rubber', 'Steel'], ordered=True), 'Undercarriage_Pad_Width': CategoricalDtype(categories=['14 inch', '15 inch', '16 inch', '18 inch', '20 inch',\n",
      "                  '22 inch', '24 inch', '25 inch', '26 inch', '27 inch',\n",
      "                  '28 inch', '30 inch', '31 inch', '31.5 inch', '32 inch',\n",
      "                  '33 inch', '34 inch', '36 inch', 'None or Unspecified'],\n",
      ", ordered=True), 'Stick_Length': CategoricalDtype(categories=['10' 10\"', '10' 2\"', '10' 6\"', '11' 0\"', '11' 10\"',\n",
      "                  '12' 10\"', '12' 4\"', '12' 8\"', '13' 10\"', '13' 7\"', '13' 9\"',\n",
      "                  '14' 1\"', '15' 4\"', '15' 9\"', '19' 8\"', '24' 3\"', '6' 3\"',\n",
      "                  '7' 10\"', '8' 10\"', '8' 2\"', '8' 4\"', '8' 6\"', '9' 10\"',\n",
      "                  '9' 2\"', '9' 5\"', '9' 6\"', '9' 7\"', '9' 8\"',\n",
      "                  'None or Unspecified'],\n",
      ", ordered=True), 'Thumb': CategoricalDtype(categories=['Hydraulic', 'Manual', 'None or Unspecified'], ordered=True), 'Pattern_Changer': CategoricalDtype(categories=['No', 'None or Unspecified', 'Yes'], ordered=True), 'Grouser_Type': CategoricalDtype(categories=['Double', 'Single', 'Triple'], ordered=True), 'Backhoe_Mounting': CategoricalDtype(categories=['None or Unspecified', 'Yes'], ordered=True), 'Blade_Type': CategoricalDtype(categories=['Angle', 'Coal', 'Landfill', 'No', 'None or Unspecified',\n",
      "                  'PAT', 'Semi U', 'Straight', 'U', 'VPAT'],\n",
      ", ordered=True), 'Travel_Controls': CategoricalDtype(categories=['1 Speed', '2 Pedal', 'Differential Steer', 'Finger Tip',\n",
      "                  'Lever', 'None or Unspecified', 'Pedal'],\n",
      ", ordered=True), 'Differential_Type': CategoricalDtype(categories=['Limited Slip', 'Locking', 'No Spin', 'Standard'], ordered=True), 'Steering_Controls': CategoricalDtype(categories=['Command Control', 'Conventional', 'Four Wheel Standard',\n",
      "                  'No', 'Wheel'],\n",
      ", ordered=True)}\n"
     ]
    }
   ],
   "source": [
    "col_type=dict(df_raw.dtypes)\n",
    "print(col_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#from fastai.torch_core import*\n",
    "\n",
    "from fastai.tabular.all import*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesID</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>MachineID</th>\n",
       "      <th>ModelID</th>\n",
       "      <th>datasource</th>\n",
       "      <th>auctioneerID</th>\n",
       "      <th>YearMade</th>\n",
       "      <th>MachineHoursCurrentMeter</th>\n",
       "      <th>UsageBand</th>\n",
       "      <th>saledate</th>\n",
       "      <th>...</th>\n",
       "      <th>saleDay</th>\n",
       "      <th>saleDayofweek</th>\n",
       "      <th>saleDayofyear</th>\n",
       "      <th>saleIs_month_end</th>\n",
       "      <th>saleIs_month_start</th>\n",
       "      <th>saleIs_quarter_end</th>\n",
       "      <th>saleIs_quarter_start</th>\n",
       "      <th>saleIs_year_end</th>\n",
       "      <th>saleIs_year_start</th>\n",
       "      <th>saleElapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139246</td>\n",
       "      <td>66000</td>\n",
       "      <td>999089</td>\n",
       "      <td>3157</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>2006-11-16</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>320</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.163635e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139248</td>\n",
       "      <td>57000</td>\n",
       "      <td>117657</td>\n",
       "      <td>77</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>4640.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>2004-03-26</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.080259e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139249</td>\n",
       "      <td>10000</td>\n",
       "      <td>434808</td>\n",
       "      <td>7009</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>2838.0</td>\n",
       "      <td>High</td>\n",
       "      <td>2004-02-26</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.077754e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1139251</td>\n",
       "      <td>38500</td>\n",
       "      <td>1026470</td>\n",
       "      <td>332</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>3486.0</td>\n",
       "      <td>High</td>\n",
       "      <td>2011-05-19</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.305763e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1139253</td>\n",
       "      <td>11000</td>\n",
       "      <td>1057373</td>\n",
       "      <td>17311</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>722.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2009-07-23</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>204</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.248307e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401120</th>\n",
       "      <td>6333336</td>\n",
       "      <td>10500</td>\n",
       "      <td>1840702</td>\n",
       "      <td>21439</td>\n",
       "      <td>149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-11-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.320192e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401121</th>\n",
       "      <td>6333337</td>\n",
       "      <td>11000</td>\n",
       "      <td>1830472</td>\n",
       "      <td>21439</td>\n",
       "      <td>149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-11-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.320192e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401122</th>\n",
       "      <td>6333338</td>\n",
       "      <td>11500</td>\n",
       "      <td>1887659</td>\n",
       "      <td>21439</td>\n",
       "      <td>149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-11-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.320192e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401123</th>\n",
       "      <td>6333341</td>\n",
       "      <td>9000</td>\n",
       "      <td>1903570</td>\n",
       "      <td>21435</td>\n",
       "      <td>149</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-25</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.319501e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401124</th>\n",
       "      <td>6333342</td>\n",
       "      <td>7750</td>\n",
       "      <td>1926965</td>\n",
       "      <td>21435</td>\n",
       "      <td>149</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-25</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.319501e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>401125 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SalesID  SalePrice  MachineID  ModelID  datasource  auctioneerID  \\\n",
       "0       1139246      66000     999089     3157         121           3.0   \n",
       "1       1139248      57000     117657       77         121           3.0   \n",
       "2       1139249      10000     434808     7009         121           3.0   \n",
       "3       1139251      38500    1026470      332         121           3.0   \n",
       "4       1139253      11000    1057373    17311         121           3.0   \n",
       "...         ...        ...        ...      ...         ...           ...   \n",
       "401120  6333336      10500    1840702    21439         149           1.0   \n",
       "401121  6333337      11000    1830472    21439         149           1.0   \n",
       "401122  6333338      11500    1887659    21439         149           1.0   \n",
       "401123  6333341       9000    1903570    21435         149           2.0   \n",
       "401124  6333342       7750    1926965    21435         149           2.0   \n",
       "\n",
       "        YearMade  MachineHoursCurrentMeter UsageBand   saledate  ... saleDay  \\\n",
       "0           2004                      68.0       Low 2006-11-16  ...      16   \n",
       "1           1996                    4640.0       Low 2004-03-26  ...      26   \n",
       "2           2001                    2838.0      High 2004-02-26  ...      26   \n",
       "3           2001                    3486.0      High 2011-05-19  ...      19   \n",
       "4           2007                     722.0    Medium 2009-07-23  ...      23   \n",
       "...          ...                       ...       ...        ...  ...     ...   \n",
       "401120      2005                       NaN       NaN 2011-11-02  ...       2   \n",
       "401121      2005                       NaN       NaN 2011-11-02  ...       2   \n",
       "401122      2005                       NaN       NaN 2011-11-02  ...       2   \n",
       "401123      2005                       NaN       NaN 2011-10-25  ...      25   \n",
       "401124      2005                       NaN       NaN 2011-10-25  ...      25   \n",
       "\n",
       "       saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start  \\\n",
       "0                  3           320            False              False   \n",
       "1                  4            86            False              False   \n",
       "2                  3            57            False              False   \n",
       "3                  3           139            False              False   \n",
       "4                  3           204            False              False   \n",
       "...              ...           ...              ...                ...   \n",
       "401120             2           306            False              False   \n",
       "401121             2           306            False              False   \n",
       "401122             2           306            False              False   \n",
       "401123             1           298            False              False   \n",
       "401124             1           298            False              False   \n",
       "\n",
       "       saleIs_quarter_end saleIs_quarter_start saleIs_year_end  \\\n",
       "0                   False                False           False   \n",
       "1                   False                False           False   \n",
       "2                   False                False           False   \n",
       "3                   False                False           False   \n",
       "4                   False                False           False   \n",
       "...                   ...                  ...             ...   \n",
       "401120              False                False           False   \n",
       "401121              False                False           False   \n",
       "401122              False                False           False   \n",
       "401123              False                False           False   \n",
       "401124              False                False           False   \n",
       "\n",
       "       saleIs_year_start   saleElapsed  \n",
       "0                  False  1.163635e+09  \n",
       "1                  False  1.080259e+09  \n",
       "2                  False  1.077754e+09  \n",
       "3                  False  1.305763e+09  \n",
       "4                  False  1.248307e+09  \n",
       "...                  ...           ...  \n",
       "401120             False  1.320192e+09  \n",
       "401121             False  1.320192e+09  \n",
       "401122             False  1.320192e+09  \n",
       "401123             False  1.319501e+09  \n",
       "401124             False  1.319501e+09  \n",
       "\n",
       "[401125 rows x 66 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_datepart(df_raw, 'saledate',drop=False, time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2006\n",
       "1    2004\n",
       "2    2004\n",
       "3    2011\n",
       "4    2009\n",
       "Name: saleYear, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.saleYear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "df_raw.to_feather('tmp/bulldozers-raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_feather('tmp/bulldozers-raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Backhoe_Mounting            322453\n",
       "Blade_Extension             375906\n",
       "Blade_Type                  321292\n",
       "Blade_Width                 375906\n",
       "Coupler                     187173\n",
       "Coupler_System              357667\n",
       "Differential_Type           331714\n",
       "Drive_System                296764\n",
       "Enclosure                      325\n",
       "Enclosure_Type              375906\n",
       "Engine_Horsepower           375906\n",
       "Forks                       209048\n",
       "Grouser_Tracks              357763\n",
       "Grouser_Type                301972\n",
       "Hydraulics                   80555\n",
       "Hydraulics_Flow             357763\n",
       "MachineHoursCurrentMeter    258360\n",
       "MachineID                        0\n",
       "ModelID                          0\n",
       "Pad_Type                    321991\n",
       "Pattern_Changer             301907\n",
       "ProductGroup                     0\n",
       "ProductGroupDesc                 0\n",
       "ProductSize                 210775\n",
       "Pushblock                   375906\n",
       "Ride_Control                252519\n",
       "Ripper                      296988\n",
       "SalePrice                        0\n",
       "SalesID                          0\n",
       "Scarifier                   375895\n",
       "Steering_Controls           331756\n",
       "Stick                       321991\n",
       "Stick_Length                301907\n",
       "Thumb                       301837\n",
       "Tip_Control                 375906\n",
       "Tire_Size                   306407\n",
       "Track_Type                  301972\n",
       "Transmission                217895\n",
       "Travel_Controls             321291\n",
       "Turbocharged                321991\n",
       "Undercarriage_Pad_Width     301253\n",
       "UsageBand                   331486\n",
       "YearMade                         0\n",
       "auctioneerID                 20136\n",
       "datasource                       0\n",
       "fiBaseModel                      0\n",
       "fiModelDesc                      0\n",
       "fiModelDescriptor           329206\n",
       "fiModelSeries               344217\n",
       "fiProductClassDesc               0\n",
       "fiSecondaryDesc             137191\n",
       "saleDay                          0\n",
       "saleDayofweek                    0\n",
       "saleDayofyear                    0\n",
       "saleElapsed                      0\n",
       "saleIs_month_end                 0\n",
       "saleIs_month_start               0\n",
       "saleIs_quarter_end               0\n",
       "saleIs_quarter_start             0\n",
       "saleIs_year_end                  0\n",
       "saleIs_year_start                0\n",
       "saleMonth                        0\n",
       "saleWeek                         0\n",
       "saleYear                         0\n",
       "saledate                         0\n",
       "state                            0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)\n",
    "display_all(df_raw.isnull().sum().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df, y, nas = proc_df(df_raw, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878597546249386"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(df, y)\n",
    "m.score(df,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
